from p1 import DIRECTIONS
import random
from p1 import POSSIBLE_ACTIONS
import copy
from p3 import calculate_value_matrix, generate_init_policy
"""
Analysis:
In this problem, we implement the Q-value temporal difference learning algorithm to find the optimal policy
in the method train() in class QValueTDLearning. The search will end when the learning rate decays small enough(0.001).

We compare the sum of values of the generated policy with the optimal policy by policy iteration to validate whether
the policy is optimal. If the difference is smaller than the max difference, we consider the policy is optimal. 
The idea is that if two sums are close enough, the generated policy achieves the maximum possible expected return 
from all states regardless of the specific actions it chooses.

By default, the optimal policy generated by policy iteration in p3 is
| E || E || E || x |
| N || # || W || x |
| N || W || W || S |
There are 3 policies in q-values and their differences with the optimal policy
0.76
| E || E || E || x |
| N || # || N || x |
| N || E || N || W |

0.94
| E || E || E || x |
| N || # || N || x |
| E || E || N || W |

0.47
| E || E || E || x |
| N || # || N || x |
| N || W || N || W |
We believe all of theme are optimal. 
For `EEExN#NxNWNW`, average 5% difference per state is considered acceptable. For `EEExN#NxEENW`, average 10% difference 
per state is somewhat higher. We think this may depend on the priority of seeking benefit or avoiding harm. For policies
of q-values (2,3), they go west to avoid the negative reward and then go north, north, east to get the positive reward.
While for `EEExN#WxNWWS`, it goes south, opposite to the negative reward, to avoid harm to the greatest extent.

We try to adjust some parameters to see if there will be some changes.
We find out that if we set the discount to 0.9, the optimal policy from policy iteration will be `EEExN#NxNENW`, 
which is the same as one of the polices generated by q-values. This time the differences between value functions 
is closest to 0, making our statement that all policies are optimal more convincing. By considering less about the
future value, the agent seems to seek benefits more.

We also test the change of the learning rate(default 0.1). If we set the learning rate to 0.4, which means the agent
will loop through more iterations and consider more about future information.
There is a new policy with a lower value function difference from q-values below, which means that it is more close to 
optimal policy. From this we can conclude that if the agent considers more about the future, it will tend to avoid harm.
0.19
| E || E || E || x |
| N || # || N || x |
| N || W || W || W |

Conclusion:
The phenomena we observed by adjusting the discount and learning rate corroborate each other. We can use `discount` and
`learning rate` to control the agent's tendency to seek benefit or avoid harm. 
If the agent considers more about the future, it will tend to avoid harm. Conversely, it will tend to seek benefit.
"""

"""
Below is the content of test case 2 in problem 3.
We set the parameter values as default values in the class QValueTDLearning init method and define a grid_matrix below.
discount: 1
noise: 0.1
livingReward: -0.01
iterations: 20
grid:
    _    _    _    1
    _    #    _   -1
    S    _    _    _
"""
grid = [
    ['_', '_', '_', '1'],
    ['_', '#', '_', '-1'],
    ['S', '_', '_', '_']
]


class QValueTDLearning:
    def __init__(self, discount=1.0, noise=0.1, living_reward=-0.01, iterations=20):
        self.grid = grid
        self.discount = discount
        self.noise = noise
        self.living_reward = living_reward
        self.iterations = iterations
        self.q_values = {}
        # exploration rate
        # controls the agentâ€™s tendency to explore the environment rather than exploit its current knowledge
        # 1.0 means the agent will start by exploring entirely, taking random actions
        self.epsilon = 1.0
        # how quickly the epsilon decreases over time.
        self.epsilon_decay = 0.99
        self.epsilon_min = 0.01
        # control the rate at which Q-values are updated based on new information
        self.learning_rate = 0.1
        self.learning_rate_decay = 0.99
        self.alpha_min = 0.001
        # the maximum difference between the sum of values and the optimal sum of values
        self.max_difference = 1.0

    def train(self):
        """train the agent using Q-learning."""
        while True:
            state = self.get_start_position()
            done = False
            while not done:
                action = self.get_next_action(state)
                next_state = self.move(state, action)
                reward = self.get_reward(next_state)
                # init Q-values of the next state
                self.init_q_values(next_state)
                # choose the action with the highest Q-value e.g. {'state': {'N': 1.0, 'S': 2.0, 'E': 3.0, 'W': 4.0}}
                best_next_action = max(self.q_values[next_state], key=self.q_values[next_state].get)
                # calculate temporal difference learning sample
                td_target = reward + self.discount * self.q_values[next_state][best_next_action]
                # update Q-value of the current state
                self.q_values[state][action] = ((1 - self.learning_rate) * self.q_values[state][action] +
                                                self.learning_rate * td_target)
                # reach a terminal state -> stop
                if self.is_number(self.grid[next_state[0]][next_state[1]]):
                    done = True
                state = next_state
            # Decay epsilon and learning rate
            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)
            self.learning_rate = self.learning_rate * self.learning_rate_decay
            # stop the search when the learning rate decays small enough
            if self.learning_rate < self.alpha_min:
                break

    def get_next_action(self, state):
        """use epsilon-greedy to explore"""
        # ensure Q-values of the current state are initialized
        self.init_q_values(state)
        # explore four directions randomly
        if random.uniform(0, 1) < self.epsilon:
            best_action = random.choice(list(DIRECTIONS.keys()))
        # exploit
        else:
            # choose the action with the highest Q-value
            best_action = max(self.q_values[state], key=self.q_values[state].get)
            # introduce noise
        return random.choices(population=POSSIBLE_ACTIONS[best_action],
                              weights=[1 - 2 * self.noise, self.noise, self.noise])[0]

    def move(self, state, action):
        """Move to the next state given the action."""
        row, col = state
        d_row, d_col = DIRECTIONS[action]
        new_row, new_col = row + d_row, col + d_col
        if 0 <= new_row < len(self.grid) and 0 <= new_col < len(self.grid[0]) and self.grid[new_row][new_col] != '#':
            return new_row, new_col
        return row, col

    def get_reward(self, next_state):
        """Return the reward of the next state."""
        row, col = next_state
        # if the cell is a number, which means exit, return the number as the reward
        if self.is_number(self.grid[row][col]):
            return float(self.grid[row][col])
        return self.living_reward

    def init_q_values(self, state):
        """Initialize the Q values for the given state."""
        if state not in self.q_values:
            # 0 for all the 4 actions
            self.q_values[state] = {action: 0.0 for action in DIRECTIONS.keys()}

    def get_start_position(self):
        for row in range(len(self.grid)):
            for col in range(len(self.grid[row])):
                if self.grid[row][col] == 'S':
                    return row, col

    def generate_policy(self):
        """generate the policy based on the Q-values."""
        policy = [['_' for _ in range(len(self.grid[0]))] for _ in range(len(self.grid))]
        for row in range(len(self.grid)):
            for col in range(len(self.grid[0])):
                if self.grid[row][col] == '#':
                    policy[row][col] = '#'
                elif self.is_number(self.grid[row][col]):
                    policy[row][col] = 'x'
                else:
                    policy[row][col] = max(self.q_values[(row, col)], key=self.q_values[(row, col)].get)
        return policy

    def validate_optimal_policy(self, policy):
        """validate the policy generated from q-values is optimal or not."""
        value_matrix = self.policy_iteration(policy)
        value_matrix_optimal = self.value_iteration()
        sum_value = sum(sum(value for value in row if self.is_number(value)) for row in value_matrix)
        sum_value_optimal = sum(sum(value for value in row if self.is_number(value)) for row in value_matrix_optimal)
        print('sum difference', abs(sum_value - sum_value_optimal))
        if abs(sum_value - sum_value_optimal) > self.max_difference:
            return False
        return True

    def policy_iteration(self, policy):
        value_matrix = [[0 for _ in range(len(grid[0]))] for _ in range(len(grid))]
        for i in range(self.iterations):
            value_matrix = self.calculate_value_matrix(i, policy, value_matrix)
        return value_matrix

    def calculate_value_matrix(self, cur_iteration, policy, value_matrix):
        new_value_matrix = copy.deepcopy(value_matrix)
        for row in range(len(policy)):
            for col in range(len(policy[0])):
                if policy[row][col] == 'x':
                    new_value_matrix[row][col] = float(self.grid[row][col])
                elif policy[row][col] == '#':
                    new_value_matrix[row][col] = '#'
                else:
                    new_value_matrix[row][col] = self.calculate_value(cur_iteration, policy[row][col],
                                                                      value_matrix, row, col)
        return new_value_matrix

    def calculate_value(self, cur_iteration, intended_action, value_matrix, row, col):
        if cur_iteration == 1:
            return self.living_reward
        value = 0
        possible_actions = POSSIBLE_ACTIONS[intended_action]
        for action in possible_actions:
            # the pos may not change because of wall, then add the value of the current pos
            next_row, next_col = self.move((row, col), action)
            if action == intended_action:
                value += (1 - 2 * self.noise) * (self.living_reward + self.discount * value_matrix[next_row][next_col])
            else:
                value += self.noise * (self.living_reward + self.discount * value_matrix[next_row][next_col])
        return value

    def value_iteration(self):
        policy = generate_init_policy(self.grid)
        # init the output value matrix
        value_matrix = [[0 for _ in range(len(self.grid[0]))] for _ in range(len(self.grid))]
        for i in range(self.iterations):
            value_matrix = calculate_value_matrix(self.grid, policy, value_matrix, i, self.discount,
                                                  self.living_reward, self.noise)
        print('optimal:')
        self.print_policy(policy)
        return value_matrix

    def print_policy(self, policy):
        policy_print = ''
        for row in policy:
            policy_print += '|' + '||'.join([f' {cell} ' for cell in row]) + '|\n'
        print(policy_print, end='')
        # print(''.join(cell for row in policy for cell in row))

    def is_number(self, s):
        try:
            float(s)
            return True
        except ValueError:
            return False


def run_multiple_times(times, discount=1.0, noise=0.1, living_reward=-0.01, iterations=20):
    """Run the training process multiple times."""
    cnt = 0
    for _ in range(times):
        # reset parameters
        q = QValueTDLearning(discount, noise, living_reward, iterations)
        q.train()
        policy = q.generate_policy()
        # count the number of times the policy is optimal
        if q.validate_optimal_policy(policy):
            cnt += 1
        q.print_policy(policy)
    print(f'Optimal rate: {cnt}/{times}')


if __name__ == '__main__':
    run_multiple_times(10, 1.0, 0.1, -0.01, 20)
    run_multiple_times(10, 0.9, 0.1, -0.01, 20)
    run_multiple_times(10, 0.8, 0.1, -0.01, 20)


